{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6722/316924810.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y['Diagnosis'] = y['Diagnosis'].map({'M': 1, 'B': 0})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "x = breast_cancer_wisconsin_diagnostic.data.features\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets\n",
    "\n",
    "# Assuming 'y' is continuous (regression problem)\n",
    "\n",
    "# Encode 'Diagnosis' column to numerical values\n",
    "y['Diagnosis'] = y['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Create a continuous DataFrame\n",
    "y_continuous = pd.DataFrame(y, columns=['Diagnosis'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_continuous, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_train_tensor = torch.FloatTensor(x_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(np.array(y_train['Diagnosis']))\n",
    "x_test_tensor = torch.FloatTensor(x_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(np.array(y_test['Diagnosis']))\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n",
    "\n",
    "\n",
    "# Combine input features and target labels into a TensorDataset\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the neural network model for regression\n",
    "class RegressionNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(RegressionNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def runModel(bs, layer1, layer2, lr):\n",
    "    # Instantiate the regression model\n",
    "    input_size = x_train_tensor.shape[1]\n",
    "    hidden_size1 = layer1\n",
    "    hidden_size2 = layer1\n",
    "    output_size = 1  # Single output node for regression\n",
    "\n",
    "    regression_model = RegressionNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "    # Define loss function and optimizer for regression\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(regression_model.parameters(), lr=lr)\n",
    "\n",
    "    # Training the regression model\n",
    "    num_epochs = 10\n",
    "    batch_size = bs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        regression_model.train()\n",
    "        for i in range(0, len(x_train_tensor), batch_size):\n",
    "            inputs = x_train_tensor[i:i+batch_size]\n",
    "            labels = y_train_tensor[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = regression_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    regression_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = regression_model(x_test_tensor)\n",
    "        mse_loss = criterion(test_outputs, y_test_tensor)\n",
    "        return mse_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size of  4  with a loss of  0.029195287764072417\n",
      "Batch size of  8  with a loss of  0.02952264230325818\n",
      "Batch size of  16  with a loss of  0.03243524648249149\n",
      "Batch size of  32  with a loss of  0.04014174184203148\n",
      "Batch size of  64  with a loss of  0.0529320468455553\n",
      "Batch size of  128  with a loss of  0.07105891964584589\n",
      "Batch size of  256  with a loss of  0.09721524353325367\n",
      "Batch size of  512  with a loss of  0.1316498640924692\n",
      "First layer size of  4  with a loss of  0.15551776701956987\n",
      "First layer size of  8  with a loss of  0.07899072889983653\n",
      "First layer size of  16  with a loss of  0.060660927325487134\n",
      "First layer size of  32  with a loss of  0.04917455017194152\n",
      "First layer size of  64  with a loss of  0.0400213914103806\n",
      "First layer size of  128  with a loss of  0.036586372021585704\n",
      "First layer size of  256  with a loss of  0.03741926873475313\n",
      "First layer size of  512  with a loss of  0.044506886050105096\n",
      "Second layer size of  4  with a loss of  0.04040754563733935\n",
      "Second layer size of  8  with a loss of  0.040509187951684\n",
      "Second layer size of  16  with a loss of  0.04044510845467448\n",
      "Second layer size of  32  with a loss of  0.040207164335995915\n",
      "Second layer size of  64  with a loss of  0.04011686087399721\n",
      "Second layer size of  128  with a loss of  0.04080216918513179\n",
      "Second layer size of  256  with a loss of  0.040568901490420105\n",
      "Second layer size of  512  with a loss of  0.04001702168956399\n",
      "Learning rate of  1e-06  with a loss of  0.3787009957432747\n",
      "Learning rate of  9.999999999999999e-06  with a loss of  0.3371391609311104\n",
      "Learning rate of  9.999999999999999e-05  with a loss of  0.10191760785877704\n",
      "Learning rate of  0.001  with a loss of  0.03995080184563994\n",
      "Learning rate of  0.01  with a loss of  0.02639815301075578\n",
      "Learning rate of  0.1  with a loss of  0.1563902205005288\n",
      "Learning rate of  1.0  with a loss of  884.034126822412\n"
     ]
    }
   ],
   "source": [
    "# experiment 1: batch size\n",
    "i = 4\n",
    "while i < 513:\n",
    "    loss = 0\n",
    "    for j in range(500):\n",
    "        loss += runModel(i, 64, 32, 0.001)\n",
    "    loss = loss / 500\n",
    "    print(\"Batch size of \", i, \" with a loss of \", loss)\n",
    "    i *= 2\n",
    "\n",
    "# experiment 2: first layer size\n",
    "i = 4\n",
    "while i < 513:\n",
    "    loss = 0\n",
    "    for j in range(500):\n",
    "        loss += runModel(32, i, 32, 0.001)\n",
    "    loss = loss / 500\n",
    "    print(\"First layer size of \", i, \" with a loss of \", loss)\n",
    "    i *= 2\n",
    "\n",
    "# experiment 3: second layer size\n",
    "i = 4\n",
    "while i < 513:\n",
    "    loss = 0\n",
    "    for j in range(500):\n",
    "        loss += runModel(32, 64, i, 0.001)\n",
    "    loss = loss / 500\n",
    "    print(\"Second layer size of \", i, \" with a loss of \", loss)\n",
    "    i *= 2\n",
    "\n",
    "# experiment 4: learning rate\n",
    "i = 0.000001\n",
    "while i < 1.1:\n",
    "    loss = 0\n",
    "    for j in range(500):\n",
    "        loss += runModel(32, 64, 32, i)\n",
    "    loss = loss / 500\n",
    "    print(\"Learning rate of \", i, \" with a loss of \", loss)\n",
    "    i *= 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
